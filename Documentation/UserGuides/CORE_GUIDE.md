# CORE Role Guide

**Welcome to Operations Tools!** This guide covers all features available to users with the CORE role.

## üéØ Your Applications

As a CORE team member, you have access to **three applications**:

1. **User App** - Basic features
2. **QA App** - Quality assurance tools
3. **Core App** - Scoring and review tools ‚≠ê

## üìã Table of Contents

### Inherited Features
1. [User App Features](#user-app-features) - Time tracking, links, profile
2. [QA App Features](#qa-app-features) - Records, similarity, analysis

### Core App Features (Your Primary Tools)
3. [Likert Scoring](#likert-scoring)
4. [Candidate Review](#candidate-review)
5. [My Assignments](#my-assignments)
6. [Review Decisions](#review-decisions)

---

## USER APP FEATURES

> üí° **Quick Summary**: You inherit all USER features. See [USER_GUIDE.md](./USER_GUIDE.md) for details.

**Available in User App**:
- ‚è±Ô∏è **Time Tracking** - Record work hours
- üîó **Links & Resources** - Access external documentation
- üë§ **Profile Management** - Update password and preferences
- üêõ **Bug Reporting** - Report issues

---

## QA APP FEATURES

> üí° **Quick Summary**: You inherit all QA features. See [QA_GUIDE.md](./QA_GUIDE.md) for details.

**Available in QA App**:
- üìä **Records Management** - View and filter records
- üîç **Similarity Search** - Find semantically similar records
- üìà **Top/Bottom 10 Review** - Quality assurance analysis
- üèÜ **Top Prompts Analysis** - Identify best-performing prompts
- üìã **Alignment Comparison** - AI-powered guideline evaluation

---

## CORE APP FEATURES

> ‚≠ê **Your Primary Workspace**: The Core App contains your scoring and review tools.

## Likert Scoring

Rate records on multiple dimensions using standardized Likert scales to provide quantitative quality assessments.

### What is Likert Scoring?

Likert scoring is a standardized rating method where you evaluate records on specific dimensions using numeric scales (typically 1-7). This creates quantitative data for analysis and comparison.

**Common Dimensions**:
- **Realism** (1-7): How realistic/believable is the content?
- **Quality** (1-7): Overall quality of the content
- **Helpfulness** (1-7): How helpful is this response?
- **Accuracy** (1-7): Factual correctness
- **Clarity** (1-7): How clear and understandable?

### Accessing Likert Scoring

1. Navigate to **Core App**
2. Click **Likert Scoring** in the sidebar
3. You'll see the scoring dashboard

### Scoring Interface

**Record Display**:
- Full record content displayed prominently
- Record metadata (type, category, project)
- Context information if available
- Previous scores (if already rated)

**Rating Scales**:
- Each dimension shows 1-7 scale
- Hover over numbers for descriptions
- Click number to assign score
- Visual indicators show current selection

**Scale Interpretation**:
- **1-2**: Very Poor/Strongly Disagree
- **3-4**: Poor to Fair/Disagree
- **5**: Neutral/Acceptable
- **6-7**: Good to Excellent/Agree

### Scoring Workflow

**Step-by-Step**:
1. **Read Content Carefully**
   - Read the entire record
   - Understand context and intent
   - Consider project guidelines

2. **Evaluate Each Dimension**
   - Rate Realism: How believable?
   - Rate Quality: Overall assessment
   - Rate additional dimensions as shown

3. **Add Comments (Optional)**
   - Explain your ratings
   - Note specific issues or strengths
   - Provide context for extreme scores

4. **Submit Score**
   - Click **"Submit"** button
   - Score is saved permanently
   - Move to next record

5. **Review Next Record**
   - System automatically loads next record
   - Progress indicator shows completion

### Scoring Best Practices

**Be Consistent**:
- Use the same criteria for all records
- Define what each number means to you
- Apply standards uniformly
- Re-calibrate periodically

**Be Objective**:
- Focus on content, not personal preference
- Follow project guidelines
- Remove bias from ratings
- Consider intended audience

**Be Thorough**:
- Read complete content before scoring
- Don't rush through records
- Take breaks to maintain focus
- Re-read if uncertain

**Document Reasoning**:
- Add comments for borderline scores
- Explain very high or very low ratings
- Note anything unusual
- Help future reviewers understand

### Scoring Calibration

**Why Calibrate?**
- Ensures consistency across raters
- Aligns understanding of scale
- Reduces rater bias
- Improves data quality

**Calibration Sessions**:
1. Group of raters score same records independently
2. Compare scores and discuss differences
3. Align on criteria and scale interpretation
4. Adjust scoring approach based on feedback

**Self-Calibration**:
- Periodically re-score past records
- Check consistency with previous scores
- Identify drift in your ratings
- Recalibrate your mental scale

### Batch Scoring

**Efficient Workflow**:
1. Filter records by project or category
2. Work through entire batch in one session
3. Maintain consistent mindset
4. Take short breaks every 20-30 records

**Progress Tracking**:
- Dashboard shows total records assigned
- Progress bar indicates completion
- Filters show scored vs. unscored
- Statistics update in real-time

### Understanding Your Scores

**View Your History**:
- Access scoring history from dashboard
- Filter by date, project, dimension
- Export to CSV for analysis
- Review patterns in your ratings

**Score Statistics**:
- Average scores per dimension
- Distribution of your ratings
- Comparison to other raters
- Consistency metrics

---

## Candidate Review

Review and evaluate candidate submissions or work products as part of assessment processes.

### What is Candidate Review?

Candidate Review is used to assess work submitted by candidates (new hires, contractors, applicants) as part of evaluation or onboarding processes.

**Review Types**:
- **Technical Assessments** - Code, designs, technical writing
- **Content Creation** - Writing samples, prompts, responses
- **Problem-Solving** - Solutions to given problems
- **Portfolio Review** - Collection of past work

### Accessing Candidate Review

1. Navigate to Core App ‚Üí **Candidate Review**
2. View list of pending reviews
3. Click candidate name to start review

### Review Interface

**Candidate Information**:
- Candidate name/ID
- Position applied for
- Submission date
- Background context

**Submission Display**:
- Full submission content
- Attachments (if any)
- Evaluation criteria
- Previous reviewer comments (if applicable)

**Evaluation Forms**:
- Structured criteria matching job requirements
- Rating scales or binary (pass/fail)
- Comment fields for detailed feedback
- Overall recommendation section

### Review Process

**1. Understand Context**
- Read position requirements
- Review evaluation criteria
- Check any special instructions
- Note focus areas

**2. Review Submission**
- Read/examine entire submission
- Take notes as you go
- Compare to examples or standards
- Identify strengths and weaknesses

**3. Evaluate Against Criteria**
Rate on each dimension:
- Technical skills
- Problem-solving ability
- Communication clarity
- Attention to detail
- Creativity/innovation
- Adherence to requirements

**4. Provide Feedback**
- Specific strengths observed
- Areas for improvement
- Examples from submission
- Actionable recommendations

**5. Make Recommendation**
- **Strong Yes** - Exceptional candidate
- **Yes** - Meets requirements well
- **Maybe** - Borderline, needs discussion
- **No** - Does not meet requirements
- **Strong No** - Significant deficiencies

**6. Submit Review**
- Save your evaluation
- Review may trigger next step in pipeline
- Candidate moves to next stage or is declined

### Review Standards

**Quality Criteria**:
- Does work meet minimum requirements?
- Is quality above/below average?
- Are there critical issues?
- Would this person succeed in role?

**Consistency**:
- Use same standards for all candidates
- Don't compare candidates directly
- Evaluate against role requirements
- Remove personal bias

**Thoroughness**:
- Review entire submission
- Check all deliverables
- Verify completeness
- Test examples (if code/demos)

### Writing Effective Feedback

**Do's**:
- ‚úÖ Be specific: "Function lacks error handling on line 23"
- ‚úÖ Be constructive: "Consider adding unit tests"
- ‚úÖ Provide examples: "See lines 45-50 for good approach"
- ‚úÖ Balance positive and negative: Note strengths too

**Don'ts**:
- ‚ùå Be vague: "Code needs work"
- ‚ùå Be harsh: "This is terrible"
- ‚ùå Compare to others: "Candidate 5 was better"
- ‚ùå Make personal comments: "Seems unmotivated"

### Confidentiality

‚ö†Ô∏è **Important**: Candidate reviews are confidential.

- Don't discuss candidates with non-reviewers
- Don't share candidate work externally
- Keep feedback professional and private
- Follow company confidentiality policies

### Review Conflicts

**Conflict of Interest**:
If you know the candidate personally:
1. Disclose the relationship
2. Request to be removed from review
3. Let manager assign alternate reviewer

**Disagreement with Other Reviewers**:
- Document your reasoning clearly
- Stick to facts and criteria
- Don't change score to match others
- Discuss in calibration meeting if needed

---

## My Assignments

View and manage work items assigned specifically to you.

### What are Assignments?

Assignments are specific tasks assigned to you by administrators or managers:
- Records to score
- Candidates to review
- Quality audits to complete
- Special projects

### Accessing Assignments

1. Navigate to Core App ‚Üí **My Assignments**
2. See list of all your current assignments
3. Filter by type, status, or due date

### Assignment Dashboard

**Assignment Cards Show**:
- Assignment title/description
- Type (Scoring, Review, Audit, etc.)
- Priority level (High, Medium, Low)
- Due date
- Current status
- Progress indicator

**Status Types**:
- **Not Started** - Haven't begun yet
- **In Progress** - Currently working on it
- **Blocked** - Waiting on something
- **Completed** - Finished and submitted
- **Overdue** - Past due date

### Working on Assignments

**Starting an Assignment**:
1. Click the assignment card
2. Review instructions and requirements
3. Click **"Start"** to mark as in progress
4. Begin your work

**Tracking Progress**:
- Update status as you work
- Add notes or comments
- Mark milestones complete
- Request help if blocked

**Completing Assignments**:
1. Finish all required work
2. Review completeness
3. Click **"Submit"** or **"Mark Complete"**
4. Assignment moves to completed status

### Managing Your Workload

**Prioritization**:
1. Sort by due date to see urgent items
2. Check priority flags
3. Address high-priority items first
4. Communicate if overloaded

**Time Management**:
- Estimate time needed for each assignment
- Schedule dedicated time blocks
- Don't let assignments go overdue
- Request deadline extensions early if needed

**Getting Help**:
- Click **"Request Help"** on assignment
- Add note describing your issue
- Manager or admin will respond
- Can also escalate via chat/email

### Notifications

**You'll be notified when**:
- New assignment is added to your queue
- Assignment is approaching due date
- Assignment becomes overdue
- Manager adds comments or instructions
- Assignment is reassigned or cancelled

---

## Review Decisions

Make final decisions on records, candidates, or work products after review processes.

### What are Review Decisions?

Review Decisions is where you make official determinations after evaluation:
- **Approve/Reject** records for publication
- **Accept/Decline** candidates
- **Pass/Fail** quality gates
- **Escalate** edge cases

### When to Use Review Decisions

**After Review**:
- Multiple people have evaluated an item
- Scores and feedback are collected
- You're designated as decision-maker
- Ready to make final determination

**Edge Cases**:
- Conflicting reviews from team
- Borderline scores needing judgment call
- Unusual situations requiring expertise
- Policy exceptions or special circumstances

### Accessing Review Decisions

1. Navigate to Core App ‚Üí **Review Decisions**
2. See queue of items awaiting decisions
3. Filter by type, priority, or age

### Decision Interface

**Item Information**:
- Item being reviewed (record, candidate, etc.)
- All previous reviews and scores
- Comments from other reviewers
- Relevant context and guidelines

**Review Summary**:
- Average scores across dimensions
- Consensus vs. disagreement indicators
- Flagged issues or concerns
- Recommendation summary

**Decision Options**:
- **Approve** - Item meets standards
- **Reject** - Item fails to meet standards
- **Approve with Conditions** - Approve if changes made
- **Escalate** - Needs higher authority decision
- **Return for Re-review** - Needs more evaluation

### Making Decisions

**1. Review All Information**
- Read the item carefully
- Review all scores and feedback
- Check guideline compliance
- Consider project context

**2. Analyze Consensus**
- Do reviewers agree?
- What's the average score?
- Are there outliers?
- What's the reasoning?

**3. Apply Judgment**
- Does item meet minimum standards?
- Are issues fixable or fundamental?
- What's the business impact?
- What's the right call?

**4. Document Decision**
- Select your decision
- Write rationale (required)
- Note any conditions
- Reference specific issues

**5. Submit Decision**
- Click **"Submit Decision"**
- Decision is final and recorded
- Stakeholders are notified
- Item moves to next stage

### Decision Principles

**Consistency**:
- Apply same standards across all items
- Follow guidelines and policies
- Don't make exceptions without justification
- Document your reasoning

**Fairness**:
- Base decisions on merit, not bias
- Give benefit of doubt when reasonable
- Apply policies uniformly
- Consider extenuating circumstances

**Transparency**:
- Document your reasoning clearly
- Explain conditional approvals
- Note why you deviated from consensus
- Make process understandable

### Handling Difficult Decisions

**Conflicting Reviews**:
- Weight based on reviewer expertise
- Look for specific, actionable feedback
- Make final judgment call
- Document why you chose your path

**Borderline Cases**:
- Review minimum requirements again
- Consider "spirit of the law" vs. "letter"
- Assess business impact of decision
- When in doubt, escalate

**High-Stakes Decisions**:
- Take extra time to review thoroughly
- Consult with manager if unsure
- Document reasoning extensively
- Escalate if consequences are severe

### Appeals Process

If decisions are appealed:
1. Review appeal justification
2. Reconsider original decision
3. Check for new information
4. Either uphold or reverse decision
5. Document appeal outcome

---

## CORE Role Workflow

### Daily Routine

**Morning** (1-2 hours):
1. Check My Assignments for urgent items
2. Complete any overdue assignments
3. Review new Review Decisions
4. Start high-priority Likert scoring

**Midday** (2-3 hours):
- Continue Likert scoring batches
- Complete candidate reviews
- Make review decisions
- Document findings

**Afternoon** (1-2 hours):
- Use QA tools for quality checks
- Review top/bottom 10 records
- Update assignment status
- Plan tomorrow's priorities

**End of Day**:
- Log time in Time Tracking
- Update assignment progress
- Submit pending decisions
- Clear notifications

### Weekly Tasks

- Complete all assigned scoring batches
- Finish all candidate reviews
- Clear review decision queue
- Participate in calibration sessions
- Update managers on progress
- Review personal metrics

### Monthly Goals

- Maintain scoring consistency
- Complete assignments on time
- Provide quality feedback
- Identify improvement patterns
- Contribute to guideline updates
- Mentor junior reviewers (if applicable)

---

## Performance & Quality Metrics

### Scoring Metrics

**Volume**:
- Records scored per day/week
- Time per record (efficiency)
- Completion rate of assignments

**Quality**:
- Inter-rater reliability (consistency with peers)
- Score distribution (avoiding bias to high/low)
- Comments quality and helpfulness

**Timeliness**:
- On-time completion rate
- Average time to complete assignments
- Response time for urgent items

### Review Metrics

**Candidate Reviews**:
- Reviews completed
- Time per review
- Quality of feedback
- Decision accuracy (if tracked)

**Decision Quality**:
- Decisions made per week
- Appeal/reversal rate
- Agreement with consensus
- Reasoning clarity

### Continuous Improvement

**Self-Assessment**:
- Review your own metrics monthly
- Identify areas for improvement
- Seek feedback from managers
- Adjust approach based on data

**Calibration**:
- Participate in calibration sessions
- Compare scores with peers
- Align understanding of standards
- Improve consistency

**Learning**:
- Study top-performing scorers
- Learn from feedback on your reviews
- Stay updated on guideline changes
- Attend training sessions

---

## Tips for CORE Success

### Efficiency

1. **Batch similar work** - Score all records from one project together
2. **Use keyboard shortcuts** - Learn shortcuts for faster navigation
3. **Set time blocks** - Dedicate 60-90 min blocks for focused scoring
4. **Minimize distractions** - Turn off notifications during scoring

### Quality

1. **Stay calibrated** - Regular calibration prevents drift
2. **Take breaks** - Fresh eyes maintain quality
3. **Document reasoning** - Future you will thank you
4. **Seek feedback** - Ask how to improve your evaluations

### Consistency

1. **Define your scale** - Write down what each number means
2. **Review past scores** - Check for consistency patterns
3. **Use checklists** - Standard criteria for each evaluation
4. **Calibrate regularly** - Don't let standards drift

### Communication

1. **Detailed feedback** - Specific, actionable, respectful
2. **Clear decisions** - Document your reasoning thoroughly
3. **Timely updates** - Respond to questions promptly
4. **Professional tone** - Always constructive, never harsh

---

## Troubleshooting

### "Cannot submit Likert score"

**Solutions**:
- Check all required dimensions are rated
- Verify comments are complete (if required)
- Refresh page and try again
- Report bug if persists

### "Assignment not loading"

**Solutions**:
- Check internet connection
- Verify assignment still exists (may have been cancelled)
- Try different browser
- Contact admin if issue continues

### "Review decision options grayed out"

**Solutions**:
- Verify you're assigned as decision-maker
- Check if decision was already made
- Ensure all reviews are complete
- Contact manager about permissions

---

## Need More Access?

As a CORE team member, you may need FLEET tools for:
- Data ingestion and project management
- Analytics and reporting
- Bonus window management

Contact your manager to discuss a FLEET role upgrade.

---

## Support & Resources

**Technical Issues**: Bug reporting feature
**Questions**: Ask your CORE lead or manager
**Training**: Request scoring/review training
**Documentation**: See [USER_GUIDE.md](./USER_GUIDE.md) and [QA_GUIDE.md](./QA_GUIDE.md)

---

**Document Version**: 1.0
**Last Updated**: February 2026
**Role**: CORE
**Access Level**: User App + QA App + Core App
