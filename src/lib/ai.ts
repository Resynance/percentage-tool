/**
 * AI SERVICE LAYER
 * Abstraction layer for interacting with LLM and Embedding models.
 * Supports multiple providers:
 *   - LM Studio (default, localhost:1234)
 *   - OpenRouter (cloud, requires API key)
 */

export type AIProvider = 'lmstudio' | 'openrouter';

export interface UsageInfo {
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
  cost?: number;
}

export interface CompletionResult {
  content: string;
  usage?: UsageInfo;
  provider: AIProvider;
}

export interface BalanceInfo {
  credits: number;
  usage: number;
  limit?: number;
}

interface ProviderConfig {
  provider: AIProvider;
  baseUrl: string;
  llmModel: string;
  embeddingModel: string;
  apiKey?: string;
}

import { prisma } from './prisma';

/**
 * Detects and returns the active AI provider configuration.
 * Priority: Database Settings > OpenRouter (if key provided in env) > LM Studio (default)
 */
async function getProviderConfig(): Promise<ProviderConfig> {
  // 1. Try to fetch settings from DB
  try {
    const settings = await prisma.systemSetting.findMany({
      where: {
        key: { in: ['ai_provider', 'ai_host', 'llm_model', 'embedding_model', 'openrouter_key'] }
      }
    });

    const getSetting = (k: string) => settings.find(s => s.key === k)?.value;

    const dbProvider = getSetting('ai_provider');
    if (dbProvider) {
      // If DB has explicit provider, use it (with fallbacks to env or defaults for standard fields)
      if (dbProvider === 'openrouter') {
        return {
          provider: 'openrouter',
          baseUrl: process.env.OPENROUTER_BASE_URL || 'https://openrouter.ai/api/v1',
          llmModel: getSetting('llm_model') || process.env.OPENROUTER_LLM_MODEL || 'anthropic/claude-3.5-sonnet',
          embeddingModel: getSetting('embedding_model') || process.env.OPENROUTER_EMBEDDING_MODEL || 'openai/text-embedding-3-small',
          apiKey: getSetting('openrouter_key') || process.env.OPENROUTER_API_KEY,
        };
      } else {
        return {
          provider: 'lmstudio',
          baseUrl: getSetting('ai_host') || process.env.AI_HOST || 'http://localhost:1234/v1',
          llmModel: getSetting('llm_model') || process.env.LLM_MODEL || 'meta-llama-3-8b-instruct',
          embeddingModel: getSetting('embedding_model') || process.env.EMBEDDING_MODEL || 'text-embedding-nomic-embed-text-v1.5',
        };
      }
    }
  } catch (e) {
    console.warn('Failed to fetch system settings:', e);
  }

  // 2. Fallback to Env vars (Existing logic)
  const openRouterKey = process.env.OPENROUTER_API_KEY;

  if (openRouterKey) {
    return {
      provider: 'openrouter',
      baseUrl: process.env.OPENROUTER_BASE_URL || 'https://openrouter.ai/api/v1',
      llmModel: process.env.OPENROUTER_LLM_MODEL || 'anthropic/claude-3.5-sonnet',
      embeddingModel: process.env.OPENROUTER_EMBEDDING_MODEL || 'openai/text-embedding-3-small',
      apiKey: openRouterKey,
    };
  }

  // Default to LM Studio
  return {
    provider: 'lmstudio',
    baseUrl: process.env.AI_HOST || 'http://localhost:1234/v1',
    llmModel: process.env.LLM_MODEL || 'meta-llama-3-8b-instruct',
    embeddingModel: process.env.EMBEDDING_MODEL || 'text-embedding-nomic-embed-text-v1.5',
  };
}

/**
 * Builds request headers based on the provider.
 */
function getHeaders(config: ProviderConfig): Record<string, string> {
  const headers: Record<string, string> = {
    'Content-Type': 'application/json',
  };

  if (config.provider === 'openrouter' && config.apiKey) {
    headers['Authorization'] = `Bearer ${config.apiKey}`;
    headers['HTTP-Referer'] = process.env.OPENROUTER_REFERER || 'http://localhost:3000';
    headers['X-Title'] = process.env.OPENROUTER_TITLE || 'Operations Tools';
  }

  return headers;
}

/**
 * Returns the current active provider name for diagnostics.
 */
export async function getActiveProvider(): Promise<AIProvider> {
  return (await getProviderConfig()).provider;
}

/**
 * Generates a vector embedding for a given string.
 * Used for semantic search and finding similar prompts/feedback.
 */
export async function getEmbedding(text: string): Promise<number[]> {
  const embeddings = await getEmbeddings([text]);
  return embeddings[0] || [];
}

/**
 * Generates vector embeddings for a batch of strings.
 * Significantly faster than individual requests for large datasets.
 * Supports both LM Studio and OpenRouter providers.
 */
export async function getEmbeddings(texts: string[]): Promise<number[][]> {
  const config = await getProviderConfig();
  const TIMEOUT_MS = 60000; // 60 second timeout for embedding requests

  // SANITIZE: Remove empty strings and handle malformed inputs to reduce tokenizer warnings.
  // This prevents the "last token is not SEP" warning from common embedding models (like Qwen).
  const sanitizedInput = texts.map(t => (typeof t === 'string' ? t.trim() : ''));

  if (sanitizedInput.every(t => t.length === 0)) {
    console.warn('[Embeddings] All inputs are empty, returning empty embeddings');
    return texts.map(() => []);
  }

  try {
    // Create abort controller for timeout
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), TIMEOUT_MS);

    console.log(`[Embeddings] Requesting ${texts.length} embeddings from ${config.provider} (model: ${config.embeddingModel})`);

    const response = await fetch(`${config.baseUrl}/embeddings`, {
      method: 'POST',
      headers: getHeaders(config),
      body: JSON.stringify({
        model: config.embeddingModel,
        input: sanitizedInput,
      }),
      signal: controller.signal,
    });

    clearTimeout(timeoutId);

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      const errorMsg = `${config.provider} embedding error (${response.status}): ${response.statusText} ${JSON.stringify(errorData)}`;
      console.error(`[Embeddings] ${errorMsg}`);
      throw new Error(errorMsg);
    }

    const data = await response.json();

    // Validate response structure
    if (!data.data || !Array.isArray(data.data)) {
      console.error(`[Embeddings] Invalid response structure from ${config.provider}:`, JSON.stringify(data).slice(0, 500));
      throw new Error(`Invalid embedding response: missing 'data' array`);
    }

    const embeddings = data.data.map((item: any) => item.embedding);

    // Validate that we got actual embeddings
    const validCount = embeddings.filter((e: any) => Array.isArray(e) && e.length > 0).length;
    if (validCount === 0) {
      console.error(`[Embeddings] No valid embeddings in response from ${config.provider}`);
      throw new Error(`No valid embeddings returned`);
    }

    console.log(`[Embeddings] Received ${validCount}/${texts.length} valid embeddings`);
    return embeddings;
  } catch (error: any) {
    if (error.name === 'AbortError') {
      console.error(`[Embeddings] Request timed out after ${TIMEOUT_MS}ms`);
    } else {
      console.error(`[Embeddings] Error from ${config.provider}:`, error.message);
    }
    // Return empty embeddings for the batch on failure
    return texts.map(() => []);
  }
}

/**
 * Chat Completion with usage tracking.
 * Returns content along with token usage and cost information (when available from OpenRouter).
 */
export async function generateCompletionWithUsage(prompt: string, systemPrompt?: string): Promise<CompletionResult> {
  const config = await getProviderConfig();

  // TRUNCATION STRATEGY:
  // Large prompts (e.g., analyzing 500+ records) can exceed context limits.
  // We apply a safe character limit (approx 20k chars ~ 5k tokens) to prevent 400 errors from LM Studio/OpenRouter.
  const MAX_PROMPT_CHARS = 20000;
  const truncatedPrompt = prompt.length > MAX_PROMPT_CHARS
    ? prompt.slice(0, MAX_PROMPT_CHARS) + "\n\n[...Truncated due to length...]"
    : prompt;

  try {
    const response = await fetch(`${config.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: getHeaders(config),
      body: JSON.stringify({
        model: config.llmModel,
        messages: [
          ...(systemPrompt ? [{ role: 'system', content: systemPrompt }] : []),
          { role: 'user', content: truncatedPrompt }
        ],
        temperature: 0.7,
      }),
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(`${config.provider} error: ${response.statusText} ${JSON.stringify(errorData)}`);
    }

    const data = await response.json();
    const content = data.choices?.[0]?.message?.content || '';

    return {
      content,
      usage: data.usage ? {
        promptTokens: data.usage.prompt_tokens || 0,
        completionTokens: data.usage.completion_tokens || 0,
        totalTokens: data.usage.total_tokens || 0,
        cost: data.usage.cost,
      } : undefined,
      provider: config.provider,
    };
  } catch (error: any) {
    console.error(`Error in completion (${config.provider}):`, error);

    const errorMessage = config.provider === 'openrouter'
      ? `Analysis Error: ${error.message}. Please verify your OpenRouter API key and that the model "${config.llmModel}" is available.`
      : `Analysis Error: ${error.message}. Please verify that LM Studio is running at ${config.baseUrl} and that the model "${config.llmModel}" is loaded.`;

    return {
      content: errorMessage,
      provider: config.provider,
    };
  }
}

/**
 * Standard Chat Completion entry point.
 * Used for summarizing trends and performing guideline alignment checks.
 * Supports both LM Studio and OpenRouter providers.
 */
export async function generateCompletion(prompt: string, systemPrompt?: string): Promise<string> {
  const result = await generateCompletionWithUsage(prompt, systemPrompt);
  return result.content;
}

/**
 * Math utility for semantic distance.
 * 1.0 = identical meaning, 0.0 = completely unrelated.
 */
export function cosineSimilarity(vecA: number[], vecB: number[]): number {
  const dotProduct = vecA.reduce((sum, a, i) => sum + a * vecB[i], 0);
  const magA = Math.sqrt(vecA.reduce((sum, a) => sum + a * a, 0));
  const magB = Math.sqrt(vecB.reduce((sum, b) => sum + b * b, 0));
  if (magA === 0 || magB === 0) return 0;
  return dotProduct / (magA * magB);
}

/**
 * Fetches the current OpenRouter API key balance.
 * Returns null if not using OpenRouter or if the request fails.
 */
export async function getOpenRouterBalance(): Promise<BalanceInfo | null> {
  const config = await getProviderConfig();

  if (config.provider !== 'openrouter' || !config.apiKey) {
    return null;
  }

  try {
    const response = await fetch('https://openrouter.ai/api/v1/auth/key', {
      headers: {
        'Authorization': `Bearer ${config.apiKey}`,
      },
    });

    if (!response.ok) {
      console.error('Failed to fetch OpenRouter balance:', response.statusText);
      return null;
    }

    const data = await response.json();
    const usage = data.data?.usage || 0;
    const limit = data.data?.limit;

    return {
      credits: limit ? limit - usage : 0,
      usage,
      limit,
    };
  } catch (error) {
    console.error('Error fetching OpenRouter balance:', error);
    return null;
  }
}
